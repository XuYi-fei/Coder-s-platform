package com.github.paicoding.forum.service.chatv2.service;

import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.ai.chat.client.ChatClient;
import org.springframework.ai.chat.metadata.ChatResponseMetadata;
import org.springframework.ai.chat.metadata.Usage;
import org.springframework.ai.chat.model.ChatResponse;
import org.springframework.stereotype.Service;
import reactor.core.publisher.Flux;

import java.time.Duration;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicLong;
import java.util.concurrent.atomic.AtomicReference;

import static org.springframework.ai.chat.memory.ChatMemory.CONVERSATION_ID;

/**
 * æµå¼èŠå¤©æœåŠ¡ - å¤„ç† SSE æµå¼å“åº”
 *
 * @author XuYifei
 * @date 2025-11-16
 */
@Slf4j
@Service
@RequiredArgsConstructor
public class StreamingChatService {

    private final ChatMemoryService chatMemoryService;
    private final ChatConversationService chatConversationService;
    private final TokenQuotaService tokenQuotaService;

    /**
     * æ‰§è¡Œæµå¼èŠå¤©è¯·æ±‚
     *
     * @param chatClient ChatClient
     * @param historyId ä¼šè¯ID
     * @param userId ç”¨æˆ·ID
     * @param modelId æ¨¡å‹ID
     * @param userMessage ç”¨æˆ·æ¶ˆæ¯
     * @return æµå¼å“åº”
     */
    public Flux<String> executeStreamingChat(ChatClient chatClient, Long historyId, Long userId, String modelId, String userMessage) {
        AtomicBoolean streamCompleted = new AtomicBoolean(false);
        AtomicLong lastContentTime = new AtomicLong(System.currentTimeMillis());
        AtomicReference<ChatResponse> lastResponse = new AtomicReference<>();

        // 1. æ›´æ–°ä¼šè¯æ—¶é—´
        chatConversationService.updateConversationTime(historyId);

        // æ³¨æ„ï¼šç”¨æˆ·æ¶ˆæ¯å’Œ AI å›å¤çš„ä¿å­˜ç”± MessageChatMemoryAdvisor è‡ªåŠ¨å¤„ç†
        // ä¸éœ€è¦æ‰‹åŠ¨è°ƒç”¨ chatMemoryService.add()

        // 2. åˆ›å»ºå¿ƒè·³æµï¼ˆå½“é•¿æ—¶é—´æ²¡æœ‰å†…å®¹æ—¶å‘é€å¿ƒè·³ï¼‰
        Flux<String> heartbeatStream = Flux.interval(Duration.ofMillis(500))
                .takeWhile(i -> !streamCompleted.get())
                .flatMap(i -> {
                    long timeSinceLastContent = System.currentTimeMillis() - lastContentTime.get();
                    if (timeSinceLastContent > 800) {
                        return Flux.just("[HEARTBEAT]");
                    }
                    return Flux.empty();
                });

        // 3. åˆ›å»ºå†…å®¹æµ
        // MessageChatMemoryAdvisor ä¼šè‡ªåŠ¨ï¼š
        //   - é€šè¿‡ conversationId å‚æ•°ä» ChatMemory åŠ è½½å†å²æ¶ˆæ¯
        //   - åœ¨è¯·æ±‚å®Œæˆåè‡ªåŠ¨ä¿å­˜ç”¨æˆ·æ¶ˆæ¯å’Œ AI å›å¤
        String conversationIdStr = String.valueOf(historyId);
        log.info("Starting streaming chat for conversationId: {}, message: {}", conversationIdStr, userMessage);

        Flux<String> contentStream = chatClient.prompt()
                .user(userMessage)
                .advisors(advisorSpec -> advisorSpec
                        .param(CONVERSATION_ID, conversationIdStr))
                .stream()
                .chatResponse()
                .doOnNext(response -> {
                    lastContentTime.set(System.currentTimeMillis());
                    // ä¿å­˜æœ€åä¸€ä¸ªå“åº”ï¼Œç”¨äºè·å– token ä½¿ç”¨ä¿¡æ¯
                    // åœ¨æµå¼å“åº”ä¸­ï¼Œusage ä¿¡æ¯é€šå¸¸åœ¨æœ€åä¸€ä¸ª chunk ä¸­
                    if (response != null) {
                        lastResponse.set(response);
                        // è°ƒè¯•ï¼šæ£€æŸ¥æ¯ä¸ªå“åº”æ˜¯å¦æœ‰ usage ä¿¡æ¯
                        if (response.getMetadata() != null && response.getMetadata().getUsage() != null) {
                            log.debug("Usage info found in chunk: {}", response.getMetadata().getUsage());
                        }
                    }
                })
                .flatMap(response -> {
                    if (response != null && response.getResult() != null
                        && response.getResult().getOutput() != null) {
                        String content = response.getResult().getOutput().getText();
                        if (content != null && !content.isEmpty()) {
                            log.debug("ğŸ“¤ Sending chunk: {}", content);
                            return Flux.just(content);
                        }
                    }
                    return Flux.empty();
                })
                .doOnComplete(() -> {
                    streamCompleted.set(true);
                    log.info("Streaming chat completed for historyId: {}", historyId);

                    // æå– token ä½¿ç”¨ä¿¡æ¯å¹¶æ›´æ–°é…é¢
                    try {
                        extractAndRecordTokenUsage(lastResponse.get(), userId, modelId, historyId);
                    } catch (Exception e) {
                        log.error("Failed to record token usage for historyId: {}", historyId, e);
                    }
                })
                .doOnError(error -> {
                    streamCompleted.set(true);
                    log.error("Error during streaming chat for historyId: {}", historyId, error);
                });

        // 4. åˆå¹¶å¿ƒè·³æµå’Œå†…å®¹æµï¼Œå¹¶æ·»åŠ å®Œæˆæ ‡è®°
        return Flux.merge(heartbeatStream, contentStream)
                .concatWith(Flux.just("[DONE]"));
    }

    /**
     * æ‰§è¡Œéæµå¼èŠå¤©è¯·æ±‚ï¼ˆç”¨äºæµ‹è¯•ï¼‰
     *
     * @param chatClient ChatClient
     * @param historyId ä¼šè¯ID
     * @param userMessage ç”¨æˆ·æ¶ˆæ¯
     * @return å“åº”å†…å®¹
     */
    public String executeChat(ChatClient chatClient, Long historyId, String userMessage) {
        // MessageChatMemoryAdvisor ä¼šè‡ªåŠ¨ä¿å­˜ç”¨æˆ·æ¶ˆæ¯å’Œ AI å›å¤
        // æ‰§è¡Œè¯·æ±‚
        String conversationIdStr = String.valueOf(historyId);
        log.info("Starting non-streaming chat for conversationId: {}", conversationIdStr);

        String response = chatClient.prompt()
                .user(userMessage)
                .advisors(advisorSpec -> advisorSpec
                        .param(CONVERSATION_ID, conversationIdStr))
                .call()
                .content();

        // æ›´æ–°ä¼šè¯æ—¶é—´
        chatConversationService.updateConversationTime(historyId);

        log.info("Non-streaming chat completed for historyId: {}", historyId);

        return response;
    }

    /**
     * æå–å¹¶è®°å½• token ä½¿ç”¨ä¿¡æ¯
     *
     * @param response ChatResponse
     * @param userId ç”¨æˆ·ID
     * @param modelId æ¨¡å‹ID
     * @param historyId ä¼šè¯ID
     */
    private void extractAndRecordTokenUsage(ChatResponse response, Long userId, String modelId, Long historyId) {
        if (response == null) {
            log.warn("âš ï¸ ChatResponse is null, cannot extract token usage for historyId: {}", historyId);
            return;
        }

        try {
            // ä» ChatResponse è·å– metadata
            ChatResponseMetadata metadata = response.getMetadata();
            if (metadata == null) {
                log.warn("âš ï¸ ChatResponseMetadata is null for historyId: {}, response: {}", historyId, response);
                return;
            }

            // è·å– token ä½¿ç”¨ä¿¡æ¯
            Usage usage = metadata.getUsage();
            if (usage == null) {
                log.warn("âš ï¸ Usage information is null for historyId: {}, metadata: {}", historyId, metadata);
                log.warn("âš ï¸ This may indicate the LLM API did not return usage info. Check if streamUsage=true is set.");
                return;
            }

            Integer promptTokens = usage.getPromptTokens() != null ? usage.getPromptTokens().intValue() : 0;
            Integer completionTokens = usage.getCompletionTokens() != null ? usage.getCompletionTokens().intValue() : 0;
            Integer totalTokens = usage.getTotalTokens() != null ? usage.getTotalTokens().intValue() : 0;

            log.info("âœ… Token usage extracted: historyId={}, promptTokens={}, completionTokens={}, totalTokens={}",
                    historyId, promptTokens, completionTokens, totalTokens);

            // å¦‚æœ totalTokens ä¸º 0ï¼Œè¯´æ˜å¯èƒ½ API æ²¡æœ‰è¿”å›æ­£ç¡®çš„ usage ä¿¡æ¯
            if (totalTokens == 0) {
                log.warn("âš ï¸ Total tokens is 0, skip quota deduction. This may indicate API compatibility issue.");
                return;
            }

            // æŸ¥æ‰¾æœ€æ–°çš„ assistant æ¶ˆæ¯ï¼ˆç”± MessageChatMemoryAdvisor è‡ªåŠ¨ä¿å­˜ï¼‰
            // æ³¨æ„ï¼šéœ€è¦ç­‰å¾… advisor ä¿å­˜å®Œæˆï¼Œè¿™é‡Œå¯èƒ½éœ€è¦çŸ­æš‚å»¶è¿Ÿ
            Long messageId = chatMemoryService.getLatestAssistantMessageId(historyId);
            if (messageId == null) {
                log.error("âŒ Cannot find latest assistant message for historyId: {}. The advisor may not have saved the message yet.", historyId);
                // å°è¯•ç­‰å¾…ä¸€ä¸‹å†é‡è¯•
                try {
                    Thread.sleep(100);
                    messageId = chatMemoryService.getLatestAssistantMessageId(historyId);
                    if (messageId == null) {
                        log.error("âŒ Still cannot find assistant message after retry for historyId: {}", historyId);
                        return;
                    }
                } catch (InterruptedException e) {
                    Thread.currentThread().interrupt();
                    log.error("Interrupted while waiting for assistant message", e);
                    return;
                }
            }

            log.info("ğŸ“ Found assistant messageId: {} for historyId: {}", messageId, historyId);

            // æ‰£é™¤é…é¢å¹¶è®°å½• token ä½¿ç”¨
            tokenQuotaService.deductQuotaAndRecord(
                    userId,
                    modelId,
                    messageId,
                    promptTokens,
                    completionTokens,
                    totalTokens
            );

            log.info("âœ… Token quota deducted successfully: historyId={}, messageId={}, totalTokens={}",
                    historyId, messageId, totalTokens);

        } catch (Exception e) {
            log.error("âŒ Failed to extract and record token usage for historyId: {}", historyId, e);
            // ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…å½±å“æ­£å¸¸æµç¨‹
        }
    }
}
